"""
Celery background‐tasks for the *relocation* workflow.

The actual optimisation logic lives in :pymod:`app.services.optimizer`
(which is still under construction).  This module is only responsible for
*orchestrating* the long‑running computation in a worker process and for
persisting the resulting move‑plan back to the database.

Having a thin “task‑layer” that only handles I/O and progress updates
makes the core optimiser easy to unit‑test (it is a pure function) while
still giving the API a reliable way to kick off asynchronous jobs.
"""

from __future__ import annotations

import logging
from datetime import datetime
from typing import Any, Mapping

from celery import states
from celery.exceptions import Ignore

from app.core.celery_app import celery_app
from app.core.database import engine
 # `RelocationMove` may not exist yet while the schema is still evolving.
try:
    from app.models import RelocationMove  # type: ignore
except ImportError:  # pragma: no cover
    RelocationMove = None  # type: ignore
from sqlmodel import Session

try:
    # The optimiser might not exist yet during the scaffolding phase.
    from app.services.optimizer import optimise_relocation  # type: ignore
except ModuleNotFoundError:  # pragma: no cover
    def optimise_relocation(  # type: ignore
        *,  # keep kwargs‑only interface
        max_moves: int = 500,
        fill_rate: float = 0.95,
        weights: Mapping[str, float] | None = None,
        **unused: Any,
    ) -> list[dict[str, Any]]:
        """
        Dummy fallback used while the real optimiser is not implemented.

        Generates an empty move‑list so that the task finishes successfully
        and the rest of the stack (Celery, API, UI) can be developed /
        tested in parallel.
        """
        logging.getLogger(__name__).warning(
            "Using *stub* optimise_relocation implementation – "
            "no moves will be generated"
        )
        return []


logger = logging.getLogger(__name__)


@celery_app.task(bind=True, name="relocation.start")
def run_relocation_task(
    self,  # Celery will provide task‑instance
    *,
    job_id: str | None = None,
    block_codes: list[str] | None = None,
    max_moves: int = 500,
    fill_rate: float = 0.95,
    weights: Mapping[str, float] | None = None,
) -> dict[str, Any]:
    """
    Fire‑and‑forget entry‑point used by the REST endpoint
    ``POST /v1/relocation/start``.

    Parameters
    ----------
    job_id:
        Optional external identifier generated by the API layer so the
        front‑end can poll for progress.
    block_codes:
        Limit the optimisation to the specified warehouse *blocks*.
        ``None`` ⇒ consider all blocks.
    max_moves:
        Upper bound for how many rows the optimiser may emit.
    fill_rate:
        Maximum allowed utilisation of shelf volume (0–1).
    weights:
        Objective‑function weights, e.g.
        ``{"age": 1.0, "turnover": 0.5, "distance": 2.0}``.

    Returns
    -------
    dict
        A small summary containing the number of generated moves and a
        timestamp – convenient for logging and unit‑tests.
    """
    started_at = datetime.utcnow()

    logger.info(
        "Relocation task started",
        extra={
            "job_id": job_id,
            "block_codes": block_codes,
            "max_moves": max_moves,
            "fill_rate": fill_rate,
            "weights": weights,
        },
    )

    try:
        moves = optimise_relocation(
            block_codes=block_codes,
            max_moves=max_moves,
            fill_rate=fill_rate,
            weights=weights or {},
        )
    except Exception as exc:  # pragma: no cover
        # Tell Celery that this task failed permanently and do not re‑queue.
        logger.exception("Relocation optimiser crashed: %s", exc)
        self.update_state(state=states.FAILURE, meta={"exc": str(exc)})
        raise Ignore()

    # Persist move plan (if any and model is available)
    if moves and RelocationMove is not None:
        with Session(engine) as ses:
            ses.add_all([RelocationMove(**m) for m in moves])
            ses.commit()
    elif moves:
        logger.warning(
            "RelocationMove model not present – generated moves will not be saved"
        )

    finished_at = datetime.utcnow()
    summary = {
        "job_id": job_id,
        "moves_generated": len(moves),
        "started_at": started_at.isoformat(),
        "finished_at": finished_at.isoformat(),
    }

    logger.info("Relocation task finished", extra=summary)
    return summary
